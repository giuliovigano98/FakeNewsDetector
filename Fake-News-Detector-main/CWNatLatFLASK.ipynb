{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:69: SyntaxWarning: invalid escape sequence '\\F'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:69: SyntaxWarning: invalid escape sequence '\\F'\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\T'\n",
      "C:\\Users\\yamin\\AppData\\Local\\Temp\\ipykernel_11448\\3307123494.py:69: SyntaxWarning: invalid escape sequence '\\F'\n",
      "  fake_news = pd.read_csv(\"data\\Fake-1.csv\", usecols=['text'])\n",
      "C:\\Users\\yamin\\AppData\\Local\\Temp\\ipykernel_11448\\3307123494.py:70: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  true_news = pd.read_csv(\"data\\True-1.csv\", usecols=['text'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the fake news detector script (version 2025-03-25 v8)\n",
      "Precomputed files not found. Training models...\n",
      "Training Naive Bayes...\n",
      "Training Random Forest...\n",
      "Training SVM...\n",
      "Models and data trained and saved successfully!\n",
      "Starting Flask server on http://127.0.0.1:5003...\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5003\n",
      " * Running on http://192.168.1.139:5003\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [27/Mar/2025 21:12:01] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/Mar/2025 21:12:02] \"GET /static/favicon.ico HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped by user.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from flask import Flask, request, render_template_string, make_response\n",
    "import sys\n",
    "import os\n",
    "import webbrowser\n",
    "import threading\n",
    "import time\n",
    "import socket\n",
    "import joblib\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import json\n",
    "from flask import render_template\n",
    "\n",
    "print(\"Running the fake news detector script (version 2025-03-25 v8)\")\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Global variables\n",
    "models = {}\n",
    "tfidf_vectorizer = None\n",
    "dataset_tfidf = None\n",
    "df = None\n",
    "\n",
    "def find_available_port(start_port=5000):\n",
    "    port = start_port\n",
    "    while True:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            try:\n",
    "                s.bind(('0.0.0.0', port))\n",
    "                return port\n",
    "            except OSError:\n",
    "                port += 1\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def train_and_load_models():\n",
    "    global models, tfidf_vectorizer, dataset_tfidf, df\n",
    "    model_files = {\n",
    "        \"Naive Bayes\": 'nb_pipeline.pkl',\n",
    "        \"Random Forest\": 'rf_pipeline.pkl',\n",
    "        \"SVM\": 'svm_pipeline.pkl'\n",
    "    }\n",
    "    data_files = ['models/dataset_tfidf.pkl', 'models/preprocessed_df.pkl']\n",
    "\n",
    "    if all(os.path.exists(f) for f in list(model_files.values()) + data_files):\n",
    "        print(\"Loading pre-trained models and data...\")\n",
    "        for name, file in model_files.items():\n",
    "            models[name] = joblib.load(file)\n",
    "        tfidf_vectorizer = models[\"Naive Bayes\"].named_steps['tfidf']\n",
    "        dataset_tfidf = joblib.load('models/dataset_tfidf.pkl')\n",
    "        df = joblib.load('models/preprocessed_df.pkl')\n",
    "    else:\n",
    "        print(\"Precomputed files not found. Training models...\")\n",
    "        try:\n",
    "            fake_news = pd.read_csv(\"data\\Fake-1.csv\", usecols=['text'])\n",
    "            true_news = pd.read_csv(\"data\\True-1.csv\", usecols=['text'])\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: 'Fake-1.csv' or 'True-1.csv' not found. Please ensure they are in the current directory.\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        fake_news['label'] = 0\n",
    "        true_news['label'] = 1\n",
    "        df = pd.concat([fake_news, true_news])\n",
    "        df = df.sample(n=5000, random_state=42)\n",
    "        df[\"text\"] = df[\"text\"].apply(clean_text)\n",
    "        X_train, _, y_train, _ = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "        nb_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', max_df=0.7, max_features=5000)), ('model', MultinomialNB())])\n",
    "        rf_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', max_df=0.7, max_features=5000)), ('model', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))])\n",
    "        svm_pipeline = Pipeline([('tfidf', TfidfVectorizer(stop_words='english', max_df=0.7, max_features=5000)), ('model', LinearSVC())])\n",
    "        \n",
    "        print(\"Training Naive Bayes...\")\n",
    "        nb_pipeline.fit(X_train, y_train)\n",
    "        print(\"Training Random Forest...\")\n",
    "        rf_pipeline.fit(X_train, y_train)\n",
    "        print(\"Training SVM...\")\n",
    "        svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "        models = {\"Naive Bayes\": nb_pipeline, \"Random Forest\": rf_pipeline, \"SVM\": svm_pipeline}\n",
    "        tfidf_vectorizer = models[\"Naive Bayes\"].named_steps['tfidf']\n",
    "        dataset_tfidf = tfidf_vectorizer.transform(df[\"text\"])\n",
    "\n",
    "        joblib.dump(nb_pipeline, 'models/nb_pipeline.pkl')\n",
    "        joblib.dump(rf_pipeline, 'models/rf_pipeline.pkl')\n",
    "        joblib.dump(svm_pipeline, 'models/svm_pipeline.pkl')\n",
    "        joblib.dump(dataset_tfidf, 'models/dataset_tfidf.pkl')\n",
    "        joblib.dump(df, 'models/preprocessed_df.pkl')\n",
    "        print(\"Models and data trained and saved successfully!\")\n",
    "\n",
    "SOURCE_CREDIBILITY = {\n",
    "    \"cnn.com\": \"High\",\n",
    "    \"nytimes.com\": \"High\",\n",
    "    \"foxnews.com\": \"Medium\",\n",
    "    \"infowars.com\": \"Low\",\n",
    "    \"breitbart.com\": \"Low\"\n",
    "}\n",
    "\n",
    "def check_source_credibility(text):\n",
    "    url_pattern = r'(https?://)?([a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)'\n",
    "    matches = re.findall(url_pattern, text)\n",
    "    for _, domain in matches:\n",
    "        domain = domain.lower()\n",
    "        for known_domain, credibility in SOURCE_CREDIBILITY.items():\n",
    "            if known_domain in domain:\n",
    "                return f\"Source Credibility: {credibility} (Detected: {known_domain})\"\n",
    "    return \"Source Credibility: Unknown (No recognizable source detected)\"\n",
    "\n",
    "def predict_news(news_article, model_choice):\n",
    "    if not models:\n",
    "        train_and_load_models()\n",
    "    cleaned_text = clean_text(news_article)\n",
    "    pipeline = models[model_choice]\n",
    "    prediction = pipeline.predict([cleaned_text])[0]\n",
    "    probabilities = pipeline.predict_proba([cleaned_text])[0] if model_choice != \"SVM\" else [0.5, 0.5]\n",
    "\n",
    "    label = \"Real\" if prediction == 1 else \"Fake\"\n",
    "    prob_fake = probabilities[0] * 100\n",
    "    prob_real = probabilities[1] * 100\n",
    "\n",
    "    tfidf = pipeline.named_steps['tfidf']\n",
    "    model = pipeline.named_steps['model']\n",
    "    tfidf_vector = tfidf.transform([cleaned_text])\n",
    "    feature_names = tfidf.get_feature_names_out()\n",
    "    tfidf_scores = tfidf_vector.toarray()[0]\n",
    "\n",
    "    if model_choice == \"Naive Bayes\":\n",
    "        fake_log_probs = model.feature_log_prob_[0]\n",
    "        word_contributions = {}\n",
    "        for idx, score in enumerate(tfidf_scores):\n",
    "            if score > 0:\n",
    "                word = feature_names[idx]\n",
    "                contribution = score * fake_log_probs[idx]\n",
    "                word_contributions[word] = contribution\n",
    "    else:\n",
    "        if model_choice == \"Random Forest\":\n",
    "            coef = model.feature_importances_\n",
    "        else:\n",
    "            coef = model.coef_[0] if len(model.coef_.shape) > 1 else model.coef_\n",
    "        word_contributions = {}\n",
    "        for idx, score in enumerate(tfidf_scores):\n",
    "            if score > 0:\n",
    "                if idx >= len(coef):\n",
    "                    continue\n",
    "                word = feature_names[idx]\n",
    "                contribution = score * coef[idx]\n",
    "                word_contributions[word] = contribution\n",
    "\n",
    "    top_fake_words = sorted(word_contributions.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    fake_words = set(word for word, _ in top_fake_words)\n",
    "\n",
    "    words = news_article.split()\n",
    "    highlighted_article = []\n",
    "    for word in words:\n",
    "        cleaned_word = clean_text(word)\n",
    "        if cleaned_word in fake_words:\n",
    "            highlighted_article.append(f'<span class=\"highlight-fake\">{word}</span>')\n",
    "        else:\n",
    "            highlighted_article.append(word)\n",
    "    highlighted_text = \" \".join(highlighted_article)\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = analyzer.polarity_scores(news_article)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "    sentiment = \"Positive\" if compound_score > 0.05 else \"Negative\" if compound_score < -0.05 else \"Neutral\"\n",
    "\n",
    "    credibility = check_source_credibility(news_article)\n",
    "\n",
    "    input_tfidf = tfidf_vectorizer.transform([cleaned_text])\n",
    "    similarities = cosine_similarity(input_tfidf, dataset_tfidf)[0]\n",
    "    top_indices = similarities.argsort()[-3:][::-1]\n",
    "    similar_articles = []\n",
    "    for idx in top_indices:\n",
    "        sim_score = similarities[idx] * 100\n",
    "        article_text = df.iloc[idx][\"text\"][:200] + \"...\"\n",
    "        article_label = \"Real\" if df.iloc[idx][\"label\"] == 1 else \"Fake\"\n",
    "        similar_articles.append(f\"<p><strong>Similar Article #{len(similar_articles)+1} (Similarity: {sim_score:.2f}%):</strong> {article_text} <br><em>Label: {article_label}</em></p>\")\n",
    "\n",
    "    word_contribution_data = [{\"word\": word, \"contribution\": float(contribution)} for word, contribution in top_fake_words]\n",
    "\n",
    "    output = f\"<p><strong>Prediction:</strong> {label}</p>\"\n",
    "    output += f\"<p><strong>Probability of being Fake:</strong> {prob_fake:.2f}%</p>\"\n",
    "    output += f\"<p><strong>Probability of being Real:</strong> {prob_real:.2f}%</p>\"\n",
    "    output += f\"<p><strong>Sentiment:</strong> {sentiment} (Compound Score: {compound_score:.2f})</p>\"\n",
    "    output += f\"<p><strong>{credibility}</strong></p>\"\n",
    "    output += \"<p><strong>Highlighted Article (highlighted parts indicate potentially fake content):</strong></p>\"\n",
    "    output += f\"<p>{highlighted_text}</p>\"\n",
    "    output += \"<h3>Top Contributing Words:</h3>\"\n",
    "    output += '<div id=\"wordChart\" style=\"width: 100%; height: 300px;\"></div>'\n",
    "    output += f'<script>var wordData = {json.dumps(word_contribution_data)};</script>'\n",
    "    output += \"<h3>Similar Articles in Dataset:</h3>\"\n",
    "    output += \"\".join(similar_articles)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    news_article = None\n",
    "    model_choice = \"Naive Bayes\"\n",
    "    output = None\n",
    "    if request.method == 'POST':\n",
    "        news_article = request.form.get('news_article', '')\n",
    "        model_choice = request.form.get('model_choice', 'Naive Bayes')\n",
    "        if news_article:\n",
    "            output = predict_news(news_article, model_choice)\n",
    "    response = make_response(render_template(\"index.html\", news_article=news_article, model_choice=model_choice, output=output))\n",
    "    response.headers['Cache-Control'] = 'no-cache, no-store, must-revalidate'\n",
    "    return response\n",
    "\n",
    "def run_flask():\n",
    "    port = find_available_port(start_port=5000)\n",
    "    url = f\"http://127.0.0.1:{port}\"\n",
    "    print(f\"Starting Flask server on {url}...\")\n",
    "    threading.Timer(0.5, lambda: webbrowser.open_new_tab(url)).start()\n",
    "    app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)\n",
    "\n",
    "# Execute everything\n",
    "train_and_load_models()\n",
    "threading.Thread(target=run_flask, daemon=True).start()\n",
    "\n",
    "# Keep the script running in Jupyter (optional)\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopped by user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
